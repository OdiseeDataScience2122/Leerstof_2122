{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f258ea8",
   "metadata": {},
   "source": [
    "# Tekstclassificatie\n",
    "\n",
    "Een veel voorkomende toepassing bij machine learning is het werken op tekstfragmenten.\n",
    "Denk bijvoorbeeld aan chatbots, spam-mail detectie, automatische call-centers, ...\n",
    "Dit wordt ook NLP of Natural Language Processing genoemd.\n",
    "De moeilijkheid bij het werken met NLP is dat de betekenis van een woord vaak sterk afhankelijk is van de context van het stuk tekst.\n",
    "Om met deze complexiteit om te gaan zijn geavanceerde machine learning technieken nodig zoals deep learning.\n",
    "Dit komt later bij Machine Learning in meer detail aan bod.\n",
    "De complexiteit kan echter sterk gereduceerd worden door te veronderstellen dat alle woorden in het tekstfragment onafhankelijk zijn van elkaar.\n",
    "In de rest van deze notebook wordt gewerkt met het typische voorbeeld over spam-detectie op inkomende mails.\n",
    "\n",
    "## Typisch voorbeeld: Spam detectie\n",
    "\n",
    "Hieronder staat een typisch voorbeeld van een spam mail.\n",
    "\n",
    "![spam-example](images\\spam1.jpg)\n",
    "\n",
    "Het doel is nu om deze mail te classificeren als spam of not spam (dit laatste wordt ook \"ham\" genoemd).\n",
    "We zoeken dus de kans dat deze tekst spam is op basis van alle woorden die erin staat.\n",
    "Hierdoor bekomen we de volgende vergelijking.\n",
    "\n",
    "$P(\\text{Spam}|w_1,w_2, \\dots w_n) = \\frac{P(w_1,w_2, \\dots w_n|\\text{Spam}) P(\\text{Spam})}{P(w_1,w_2, \\dots w_n)}$\n",
    "\n",
    "Na toepassing van de Bayes rule kunnen we dit ook schrijven als:\n",
    "\n",
    "$P(\\text{Spam}|w_1,w_2, \\dots w_n) = \\frac{P(w_1|w_2, \\dots w_n,\\text{Spam})P(w_2|w_3, \\dots w_n,\\text{Spam})\\dots P(\\text{Spam})}{P(w_1,w_2, \\dots w_n)}$\n",
    "\n",
    "De kansen in de teller van de breuk geven de kansen weer om een bepaald woord te vinden op basis van de andere woorden in de tekst.\n",
    "Dit is heel lastig te berekenen en zorgt voor een heel hoge complexiteit.\n",
    "Hier komt echter de veronderstelling van onafhankelijkheid van de woorden in de tekst goed van pas om deze vergelijking te vereenvoudigen.\n",
    "We bekomen namelijk:\n",
    "\n",
    "$P(\\text{Spam}|w_1,w_2, \\dots w_n) = \\frac{P(w_1|\\text{Spam})P(w_2|\\text{Spam})\\dots P(w_n|\\text{Spam})P(\\text{Spam})}{P(w_1,w_2, \\dots w_n)} = \\frac{P(\\text{Spam})\\prod \\limits_{i=1}^{n}P(w_i|\\text{Spam})}{P(w_1,w_2, \\dots w_n)}$\n",
    "\n",
    "De noemer hierin zorgt ervoor dat de teller terug omgezet wordt naar een kans (met een waarde tussen 0 en 1).\n",
    "Deze is echter onafhankelijk van de kans of het spam is of niet.\n",
    "We hebben ook niet de exacte kans nodig maar moeten gewoon weten of de teller het grootst is als het spam is of niet.\n",
    "Daarom kan de noemer dus weggelaten worden met als volgende resultaat.\n",
    "\n",
    "$P(\\text{Spam}|w_1,w_2, \\dots w_n) \\propto P(\\text{Spam})\\prod \\limits_{i=1}^{n}P(w_i|\\text{Spam})$\n",
    "\n",
    "Stel dat we de volgende gegevens hebben over een dataset met 300 spam mails en 850 ham mails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae6f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd34dc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Woord</th>\n",
       "      <th>Spam frequentie</th>\n",
       "      <th>Spam kans</th>\n",
       "      <th>Ham frequentie</th>\n",
       "      <th>Ham kans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customer</td>\n",
       "      <td>100</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advise</td>\n",
       "      <td>50</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>70</td>\n",
       "      <td>0.082353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa</td>\n",
       "      <td>120</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.035294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>money</td>\n",
       "      <td>60</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>450</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>number</td>\n",
       "      <td>180</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>660</td>\n",
       "      <td>0.776471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Woord  Spam frequentie  Spam kans  Ham frequentie  Ham kans\n",
       "0  customer              100   0.333333             200  0.235294\n",
       "1    advise               50   0.166667              70  0.082353\n",
       "2    Africa              120   0.400000              30  0.035294\n",
       "3     money               60   0.200000             450  0.529412\n",
       "4    number              180   0.600000             660  0.776471"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aantal_spam_mails = 300\n",
    "aantal_ham_mails = 850\n",
    "\n",
    "woord = [\"customer\", \"advise\", \"Africa\", \"money\", \"number\"]\n",
    "spam_freq = [100,50, 120,60, 180]\n",
    "spam_prob = [x/aantal_spam_mails for x in spam_freq]\n",
    "ham_freq = [200, 70, 30, 450, 660]\n",
    "ham_prob =  [x/aantal_ham_mails for x in ham_freq]\n",
    "\n",
    "df = pd.DataFrame({\"Woord\": woord, \"Spam frequentie\": spam_freq, \"Spam kans\": spam_prob, \"Ham frequentie\": ham_freq, \"Ham kans\": ham_prob})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860e81a",
   "metadata": {},
   "source": [
    "Beantwoord nu de volgende vragen:\n",
    "* Wat is $P(\\text{Spam})$, de kans dat een willekeurige mail spam is in de dataset?\n",
    "* Doe een manuele classificatie van de zin \"Africa advise money\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9039a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2608695652173913 0.7391304347826086\n",
      "Africa advise money is spam\n"
     ]
    }
   ],
   "source": [
    "pSpam = 300 / (850+300)\n",
    "pHam = 1 - pSpam\n",
    "print(pSpam, pHam)\n",
    "\n",
    "pTextIsSpam = pSpam * 0.4 * 0.17 * 0.2\n",
    "pTextIsHam = pHam * 0.03 * 0.08 * 0.53\n",
    "\n",
    "if(pTextIsSpam > pTextIsHam):\n",
    "    print(\"Africa advise money is spam\")\n",
    "else:\n",
    "    print(\"Africa advise money is ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec642a",
   "metadata": {},
   "source": [
    "**Hoe wordt er omgegaan met classificatie van woorden die niet in de dataset zaten?**\n",
    "\n",
    "Indien een mail uit de testset een woord bevat die nooit gezien was in de trainingsset, dan kan deze mail niet geclassificeerd worden omdat de kans van het woord steeds 0 is.\n",
    "Er zijn twee mogelijkheden om hiermee om te gaan. \n",
    "* Ofwel laat je die eruit vallen maar dan verlies je informatie.\n",
    "* Ofwel geef je deze woorden toch een bepaalde kans.\n",
    "\n",
    "Dit laatste kan gebeuren door middel van Laplacian Smoothing.\n",
    "De formule hiervoor is: \n",
    "\n",
    "$P(w) = \\frac{C(w) + \\alpha}{N+\\alpha V}$\n",
    "\n",
    "met \n",
    "* P(w) de uiteindelijke kans van het woord\n",
    "* C(w) het aantal mails waarin het woord voorkomt\n",
    "* N het aantal spam-mails\n",
    "* V het aantal verschillende woorden (features) in de dataset\n",
    "* $\\alpha$ een hyperparameter dat aangeeft hoeveel gewicht de ongeziene woorden moeten krijgen.\n",
    "    * Kleine waarde $\\rightarrow$ klein belang van ongeziene woorden $\\rightarrow$ neiging tot overfitting\n",
    "    * Grote waarde $\\rightarrow$ groot belang van ongeziene woorden $\\rightarrow$ neiging tot underfitting \n",
    "    \n",
    "Het algemene concept van Laplacian smoothing is om voor ongeziene woorden een extra fictieve mail toe te voegen die enkel bestaat uit dit woord.\n",
    "De $\\alpha$ is dan hoeveel keer deze mail wordt toegevoegd.\n",
    "Bereken voor een $\\alpha=2$ opnieuw de kansen in het dataframe df.\n",
    "Voeg hier ook een lijn aan toe voor woorden die niet in de dataset aanwezig zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bereken de geupdate matrix met de kansen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e85a6",
   "metadata": {},
   "source": [
    "Voer nu opnieuw manuele classificatie uit voor de zin \"Europe advise money\" door gebruik te maken van Laplacian Smoothing.\n",
    "Is deze zin Spam of Ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ca9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154453b3",
   "metadata": {},
   "source": [
    "**Probleem van veel kansen te vermenigvuldigen**\n",
    "\n",
    "Naast het probleem van ongeziene woorden, kan er nog een probleem optreden bij het werken met de kansberekeningen.\n",
    "Aangezien kansen een waarde tussen 0 en 1 zijn kan het zijn dat er bij vermenigvuldiging van veel kansen een floating-point underflow optreed omdat het resultaat steeds kleiner en kleiner gaat worden.\n",
    "Om dit tegen te gaan kan men gebruik maken van het logaritme van de kansen.\n",
    "Dit wordt ook **log likelihood** genoemd en wordt berekend als volgt:\n",
    "\n",
    "$log(P(\\text{Spam}|w_1,w_2, \\dots w_n)) \\propto log(P(\\text{Spam})) + \\sum\\limits_{i=1}^{n}log(P(w_i|\\text{Spam}))$\n",
    "\n",
    "De resulterende klasse is nog steeds de klasse met de hoogste log-likelihood.\n",
    "\n",
    "Bereken de log-likelihood voor de zin \"Europe advice money\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4fc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69fd2291",
   "metadata": {},
   "source": [
    "**Probleem van gelijkaardige woorden**\n",
    "\n",
    "Daarnaast is het duidelijk dat elke vorm van geschreven tekst een groot aantal overbodige woorden betekenen. \n",
    "Dit zijn dan bijvoorbeeld woorden die in heel veel zinnen voorkomen zoals ik, hij, en, daar, ...\n",
    "Deze woorden gaan geen informatie geven om de classificatie uit te voeren en kunnen dus ook genegeerd worden.\n",
    "Daarnaast kan men ook de vraag stellen over vervoegingen van werkwoorden of meervouden een apart woord moeten zijn.\n",
    "\n",
    "\n",
    "Over het algemeen gezien kunnen we de volgende stappen uitvoeren om de tekst bruikbaarder te maken:\n",
    "* Html/xml/... tags verwijderen indien er zijn ([BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/))\n",
    "* Cijfers/speciale symbolen verwijderen\n",
    "* Alles omzetten naar lowercase\n",
    "* Stopwoorden verwijderen ([nltk.corpus](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)) (vergeet niet de stopwords te downloaden indien nodig)\n",
    "* Alle woorden herleiden naar hun stam ([nltk SnowballStemmer](https://www.nltk.org/_modules/nltk/stem/snowball.html))\n",
    "* Verwijder te korte woorden\n",
    "\n",
    "Meer informatie over de natural language toolkit nltk kan je [hier](https://devopedia.org/natural-language-toolkit) vinden.\n",
    "\n",
    "Deze stappen kunnen door middel van de volgende lijnen code uitgevoerd worden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504d72ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: jensbaetensodisee\n",
      "Your Kaggle Key: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                      | 0.00/5.69M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spam-classification-for-basic-nlp.zip to .\\spam-classification-for-basic-nlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5.69M/5.69M [00:03<00:00, 1.65MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "od.download(\"https://www.kaggle.com/chandramoulinaidu/spam-classification-for-basic-nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d0812",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72807a33",
   "metadata": {},
   "source": [
    "Nu dat de tekst in zijn meest bruikbare vorm staat kan de \"bag of words\" bepaald worden van de dataset.\n",
    "Dit gaat alle unieke woorden in de volledige dataset gaan oplijsten en komt overeen met de feature vector.\n",
    "Er zijn dan twee manieren om de classificatie uit te voeren.\n",
    "* [Multi-variate Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html): Kijk enkel of het woord in de bag-of-words voorkomt in de tekst of niet (0 of 1)\n",
    "* [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html): Tel hoeveel keer elk woord in de bag-of-words voorkomt in de tekst. [CountVectorizer](https://www.educative.io/edpresso/countvectorizer-in-python) of [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
    "\n",
    "De TfidfTransformer is een speciale manier om de frequentie te berekenen, namelijk door het te vermenigvuldigen met het logaritme van het percentage van de observaties waarin het woord voorkomt.\n",
    "Deze frequentie wordt de term frequency-inverse document frequency genoemd en zorgt ervoor dat woorden die veel voorkomen minder grote waarden krijgen dan woorden die zelden voorkomen. \n",
    "\n",
    "$\\text{tfidf}_{i,j} = \\text{tf}_{i,j} \\times \\log(\\frac{N}{\\text{df}_i})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73a737",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e44d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef84195f",
   "metadata": {},
   "source": [
    "Op deze verwerkte data kan dan een classifier getrained worden zoals LogisticRegression, SVM of naive bayes.\n",
    "Merk op dat de bovenstaande vormen van naive bayes niet de enige zijn.\n",
    "Een derde interessante versie is de [ComplementNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html) methode. \n",
    "Deze gaat vooral goed presteren wanneer de dataset sterk uit balans is.\n",
    "Meer informatie over alle mogelijke implementaties van Naive Bayes vind je [hier](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e545b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c15eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
